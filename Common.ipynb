{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# from tabulate import tabulate\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_mat_list (mat, mat_labels):\n",
    "    assert (len(mat) == len(mat_labels))\n",
    "    class_label_dict = {}\n",
    "    class_mats = []\n",
    "    class_label_idx = 0\n",
    "    class_labels = []\n",
    "    for samp_idx in range (len (mat)):\n",
    "        class_label = mat_labels[samp_idx]\n",
    "        if not class_label in class_label_dict:\n",
    "            class_label_dict[class_label] = class_label_idx\n",
    "            class_labels.append (class_label)\n",
    "            class_mats.append (mat[samp_idx])\n",
    "            class_label_idx += 1\n",
    "        else:\n",
    "            class_idx = class_label_dict[class_label]\n",
    "            class_mats[class_idx] = np.vstack ([class_mats[class_idx],mat[samp_idx]])\n",
    "    return (np.array(class_mats),class_labels)\n",
    "\n",
    "def list_to_contig_mat (data_matrix_list, class_vals):\n",
    "    data_mat_contig = np.vstack (data_matrix_list)\n",
    "    class_vals_vec_list = []\n",
    "    for class_idx in range (len(data_matrix_list)):\n",
    "        class_vals_vec_list += [class_vals[class_idx]] * len (data_matrix_list[class_idx])\n",
    "    class_vals_contig = np.asarray(class_vals_vec_list)\n",
    "    return (data_mat_contig,class_vals_contig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize (train, test):\n",
    "    norm_train_set = train.copy() \n",
    "    mins, maxs = normalize_by_columns (norm_train_set)\n",
    "    norm_test_set = test.copy() \n",
    "    normalize_by_columns (norm_test_set, mins, maxs)\n",
    "    return (norm_train_set, norm_test_set)\n",
    "\n",
    "def normalize_by_columns ( full_stack, mins = None, maxs = None ):\n",
    "    \"\"\"This is a global function to normalize a matrix by columns.\n",
    "    If numpy 1D arrays of mins and maxs are provided, the matrix will be normalized against these ranges\n",
    "    Otherwise, the mins and maxs will be determined from the matrix, and the matrix will be normalized\n",
    "    against itself. The mins and maxs will be returned as a tuple.\n",
    "    Out of range matrix values will be clipped to min and max (including +/- INF)\n",
    "    zero-range columns will be set to 0.\n",
    "    NANs in the columns will be set to 0.\n",
    "    The normalized output range is hard-coded to 0-100\n",
    "    \"\"\"\n",
    "    # Edge cases to deal with:\n",
    "    # Range determination:\n",
    "    # 1. features that are nan, inf, -inf\n",
    "    # max and min determination must ignore invalid numbers\n",
    "    # nan -> 0, inf -> max, -inf -> min\n",
    "    # Normalization:\n",
    "    # 2. feature values outside of range\n",
    "    # values clipped to range (-inf to min -> min, max to inf -> max) - leaves nan as nan\n",
    "    # 3. feature ranges that are 0 result in nan feature values\n",
    "    # 4. all nan feature values set to 0\n",
    "\n",
    "    # Turn off numpy warnings, since we're taking care of invalid values explicitly\n",
    "    oldsettings = np.seterr(all='ignore')\n",
    "    if (mins is None or maxs is None):\n",
    "        # mask out NANs and +/-INFs to compute min/max\n",
    "        full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
    "        maxs = full_stack_m.max (axis=0)\n",
    "        mins = full_stack_m.min (axis=0)\n",
    "\n",
    "    # clip the values to the min-max range (NANs are left, but +/- INFs are taken care of)\n",
    "    full_stack.clip (mins, maxs, full_stack)\n",
    "    # remake a mask to account for NANs and divide-by-zero from max == min\n",
    "    full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
    "\n",
    "    # Normalize\n",
    "    full_stack_m -= mins\n",
    "    full_stack_m /= (maxs - mins)\n",
    "    # Left over NANs and divide-by-zero from max == min become 0\n",
    "    # Note the deep copy to change the numpy parameter in-place.\n",
    "    full_stack[:] = full_stack_m.filled (0) * 100.0\n",
    "\n",
    "    # return settings to original\n",
    "    np.seterr(**oldsettings)\n",
    "\n",
    "    return (mins,maxs)\n",
    "\n",
    "def standardize (train, test):\n",
    "    scaler = StandardScaler()\n",
    "    new_train_set = scaler.fit_transform(train)\n",
    "    new_test_set = scaler.transform(test)\n",
    "    return (new_train_set,new_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def round_robin_iteration (index, data_matrix_list):\n",
    "    '''Does a leave N out, where N is the number of classes.\n",
    "    The class with the smallest number of samples -1 (nsamples - 1) determines training set size.\n",
    "    Picks nsamples-1 for training and testing from a circular list starting at index.\n",
    "    Index ranges from 0 to the product of number of samples in each class.\n",
    "    data_matrix_list is a list of data matrixes, with one matrix per class'''\n",
    "    lengths = [m.shape[0] for m in data_matrix_list]\n",
    "    steps = [0]*len(lengths)\n",
    "    steps[0] = 1\n",
    "    for idx in range (1,len(lengths)):\n",
    "        steps[idx] = lengths[idx-1]*steps[idx-1]\n",
    "    nclasses = len(lengths)\n",
    "    max_samples = min (lengths) - 1\n",
    "    indexes = [0] * nclasses\n",
    "    last_index = index\n",
    "\n",
    "    for index_idx in range (nclasses-1,0,-1):\n",
    "        cur_index = last_index / steps[index_idx]\n",
    "        indexes[index_idx] = cur_index\n",
    "        last_index -= (cur_index * steps[index_idx])\n",
    "        if last_index < 0: break\n",
    "    if last_index > 0: indexes[0] = last_index\n",
    "\n",
    "    train_mats = []\n",
    "    test_mats = []\n",
    "    for class_num in range(nclasses):\n",
    "        class_indexes = [ (count+indexes[class_num]+1) % lengths[class_num] for count in range (max_samples) ]\n",
    "        train_mats.append (np.take (data_matrix_list[class_num], class_indexes, axis=0) )\n",
    "        test_mats.append (np.take (data_matrix_list[class_num], [indexes[class_num]], axis=0) )\n",
    "    return (train_mats, test_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Fisher(train_mat, train_vals):\n",
    "    \"\"\"Takes a FeatureSet_Discrete as input and calculates a Fisher score for\n",
    "    each feature. Returns a newly instantiated instance of FisherFeatureWeights.\n",
    "\n",
    "    For:\n",
    "    N = number of classes\n",
    "    F = number of features\n",
    "    It = total number of images in training set\n",
    "    Ic = number of images in a given class\n",
    "    \"\"\"\n",
    "\n",
    "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
    "    oldsettings = np.seterr(all='ignore')\n",
    "    (class_mats, class_vals) = get_class_mat_list (train_mat, train_vals)\n",
    "\n",
    "    # 1D matrix 1 * F\n",
    "    population_means = np.mean( train_mat, axis = 0 )\n",
    "    n_classes = class_mats.shape[0]\n",
    "    n_features = train_mat.shape[1]\n",
    "\n",
    "    # 2D matrix shape N * F\n",
    "    intra_class_means = np.empty( [n_classes, n_features] )\n",
    "    # 2D matrix shape N * F\n",
    "    intra_class_variances = np.empty( [n_classes, n_features] )\n",
    "\n",
    "    class_index = 0\n",
    "    for class_feature_matrix in class_mats:\n",
    "        intra_class_means[ class_index ] = np.mean( class_feature_matrix, axis=0 )\n",
    "    # Note that by default, numpy divides by N instead of the more common N-1, hence ddof=1.\n",
    "        intra_class_variances[ class_index ] = np.var( class_feature_matrix, axis=0, ddof=1 )\n",
    "        class_index += 1\n",
    "\n",
    "    # 1D matrix 1 * F\n",
    "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
    "    # for the record, in numpy:\n",
    "    # 1./0. = inf, 0./inf = 0., 1./inf = 0. inf/0. = inf, inf/inf = nan\n",
    "    # 0./0. = nan, nan/0. = nan, 0/nan = nan, nan/nan = nan, nan/inf = nan, inf/nan = nan\n",
    "    # We can't deal with NANs only, must also deal with pos/neg infs\n",
    "    # The masked array allows for dealing with \"invalid\" floats, which includes nan and +/-inf\n",
    "    denom = np.mean( intra_class_variances, axis = 0 )\n",
    "    denom[denom == 0] = np.nan\n",
    "    feature_weights_m = np.ma.masked_invalid (\n",
    "            ( np.square( population_means - intra_class_means ).sum( axis = 0 ) /\n",
    "        (n_classes - 1) ) / denom\n",
    "        )\n",
    "    # return numpy error settings to original\n",
    "    np.seterr(**oldsettings)\n",
    "\n",
    "    # the filled(0) method of the masked array sets all nan and infs to 0\n",
    "    fisher_values = feature_weights_m.filled(0).tolist()\n",
    "\n",
    "    return (fisher_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pearson(train_mat, train_vals):\n",
    "    \"\"\"Calculate regression parameters and correlation statistics that fully define\n",
    "    a continuous classifier.\n",
    "\n",
    "    At present the feature weights are proportional the Pearson correlation coefficient\n",
    "    for each given feature.\"\"\"\n",
    "\n",
    "    from scipy import stats\n",
    "\n",
    "    # Known issue: running stats.linregress() with np.seterr (all='raise') has caused\n",
    "    # arithmetic underflow (FloatingPointError: 'underflow encountered in stdtr' )\n",
    "    # I think this is something we can safely ignore in this function, and return settings\n",
    "    # back to normal at the end. -CEC\n",
    "    np.seterr (under='ignore')    \n",
    "\n",
    "    pearson_coeffs = np.zeros(train_mat.shape[1])\n",
    "\n",
    "    for feature_index in range( train_mat.shape[1] ):\n",
    "        slope, intercept, pearson_coeff, p_value, std_err = stats.linregress(\n",
    "            train_vals, train_mat[:,feature_index]\n",
    "        )\n",
    "\n",
    "        pearson_coeffs[feature_index] = pearson_coeff\n",
    "# We're just returning the pearsons^2 now...\n",
    "#    pearson_values = [val*val / r_val_squared_sum for val in pearson_coeffs ]\n",
    "#    pearson_coeffs = (pearson_coeffs * pearson_coeffs) / r_val_squared_sum\n",
    "    pearson_coeffs *= pearson_coeffs\n",
    "    \n",
    "\n",
    "    # Reset numpy\n",
    "    np.seterr (all='raise')\n",
    "\n",
    "    return pearson_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_by_weight (the_mat, feature_weights):\n",
    "    i = np.argsort(feature_weights)\n",
    "    sort_mat = the_mat[:,i]\n",
    "    sort_mat = np.fliplr(sort_mat)\n",
    "    return (sort_mat)\n",
    "\n",
    "def weigh_sort(train, test, feature_weights):\n",
    "    weigh_train = np.multiply (train, feature_weights)\n",
    "    weigh_test = np.multiply (test, feature_weights)\n",
    "\n",
    "    sorted_train = sort_by_weight (weigh_train, feature_weights)\n",
    "    sorted_test = sort_by_weight (weigh_test, feature_weights)\n",
    "    return (sorted_train, sorted_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def marg_prob_to_pred_value (marg_probs, class_vals):\n",
    "    weighted = np.array(marg_probs)*np.array(class_vals)\n",
    "    return (np.sum(weighted))\n",
    "\n",
    "def WND5 (contig_train_mat, contig_test_mat, contig_train_vals, **kwargs):\n",
    "    n_test_samples = contig_test_mat.shape[0]\n",
    "    n_train_samples = contig_train_mat.shape[0]\n",
    "    predicted_classes = np.zeros(n_test_samples)\n",
    "    predicted_values = np.zeros(n_test_samples)\n",
    "    \n",
    "    epsilon = np.finfo( np.float ).eps\n",
    "    testimg_idx = 0\n",
    "    trainimg_idx = 0\n",
    "    \n",
    "    for testimg_idx in range( n_test_samples ):\n",
    "        # initialize\n",
    "        class_dists = {}\n",
    "        class_counts = {}\n",
    "        classnames_list = []\n",
    "\n",
    "        for trainimg_idx in range( n_train_samples ):\n",
    "            train_class_label = contig_train_vals[trainimg_idx]\n",
    "            if not train_class_label in class_dists:\n",
    "                class_dists [train_class_label] = 0.0\n",
    "                class_counts[train_class_label] = 0.0\n",
    "                classnames_list.append (train_class_label)\n",
    "\n",
    "            dists = np.absolute (contig_train_mat [trainimg_idx] - contig_test_mat [testimg_idx])\n",
    "            w_dist = np.sum( dists )\n",
    "            if w_dist > epsilon:\n",
    "                class_counts[train_class_label] += 1.0\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            w_dist = np.sum( np.square( dists ) )\n",
    "            # The exponent -5 is the \"5\" in \"WND5\"\n",
    "            class_dists[ train_class_label ] += w_dist ** -5\n",
    "\n",
    "        \n",
    "        class_idx = 0\n",
    "        class_similarities = [0]*len(class_dists)\n",
    "        for class_label in classnames_list:\n",
    "            class_similarities[class_idx] = class_dists[class_label] / class_counts[class_label]\n",
    "            class_idx += 1\n",
    "\n",
    "        norm_factor = sum( class_similarities )\n",
    "        marg_probs = np.array( [ x / norm_factor for x in class_similarities ] )\n",
    "\n",
    "        predicted_class_idx = marg_probs.argmax()\n",
    "\n",
    "        predicted_classes[testimg_idx] = classnames_list[ predicted_class_idx ]\n",
    "        predicted_values[testimg_idx] = marg_prob_to_pred_value (marg_probs, classnames_list)\n",
    "\n",
    "    return (predicted_classes, predicted_values)\n",
    "\n",
    "def WND5_Cls (contig_train_mat, contig_test_mat, contig_train_vals, **kwargs):\n",
    "    predicted_classes, predicted_values = WND5 (contig_train_mat, contig_test_mat, contig_train_vals, **kwargs)\n",
    "    return (predicted_classes)\n",
    "\n",
    "def WND5_Reg (contig_train_mat, contig_test_mat, contig_train_vals, **kwargs):\n",
    "    predicted_classes, predicted_values = WND5 (contig_train_mat, contig_test_mat, contig_train_vals, **kwargs)\n",
    "    return (predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_forest_clf (contig_train_mat, contig_test_mat, contig_train_vals, **kwargs):\n",
    "    if kwargs is not None and 'rnd_state' in kwargs:\n",
    "        rnd_state = kwargs['rnd_state']\n",
    "    else:\n",
    "        rnd_state = None\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators = 30, random_state = rnd_state)\n",
    "    clf.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted_classes = clf.predict(contig_test_mat)\n",
    "    return (predicted_classes)\n",
    "\n",
    "def rand_forest_reg (contig_train_mat, contig_test_mat, contig_train_vals, **kwargs):\n",
    "    if kwargs is not None and 'rnd_state' in kwargs:\n",
    "        rnd_state = kwargs['rnd_state']\n",
    "    else:\n",
    "        rnd_state = None\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    forest = RandomForestRegressor(n_estimators=30, random_state = rnd_state)\n",
    "    forest.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted = forest.predict(contig_test_mat)\n",
    "    return (predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lin_reg(contig_train_mat, contig_test_mat, contig_train_vals, **kwargs):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted = lin_reg.predict(contig_test_mat)\n",
    "    return (predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def R2_vs_N_features (regressor, contig_mat,mat_vals,feat_range):\n",
    "    niter = contig_mat.shape[0]\n",
    "    pred_vals = np.zeros ([niter,len(feat_range)])\n",
    "    print ('Train class sizes : {}'.format(contig_mat.shape[0]-1))\n",
    "    print ('N Features        : {}-{}'.format(feat_range[0],feat_range[-1]))\n",
    "    print ('Iterations        : {}'.format(niter))\n",
    "    actual = []\n",
    "    for iter_idx in range ( niter ):\n",
    "    # Split\n",
    "        contig_train_mat = np.delete(contig_mat,[iter_idx],axis=0)\n",
    "        contig_test_mat = np.asarray([contig_mat[iter_idx]])\n",
    "\n",
    "        contig_train_vals = np.delete (mat_vals, [iter_idx])\n",
    "        contig_test_vals = np.asarray ([mat_vals[iter_idx]])\n",
    "\n",
    "        # Normalize\n",
    "        norm_train, norm_test = normalize (contig_train_mat, contig_test_mat)\n",
    "\n",
    "        # Reduce\n",
    "        feature_weights = Pearson(norm_train, contig_train_vals)\n",
    "        sorted_train, sorted_test = weigh_sort (norm_train, norm_test, feature_weights)\n",
    "    #    print (sorted_train, sorted_test)\n",
    "\n",
    "        # Classify with different numbers of features\n",
    "        nfeatures_idx = 0\n",
    "        for nfeatures in feat_range:\n",
    "            # Classify\n",
    "    #        preds,pred_val = WND5(sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals)\n",
    "    #        pred_val = rand_forest_reg (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals, iter_idx*nfeatures)\n",
    "     #       pred_val = lin_reg (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals)\n",
    "            pred_val = regressor (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals, rnd_state = iter_idx*nfeatures)\n",
    "    #        print ('nfeatures: {}, iter_idx: {}'.format(nfeatures,iter_idx))\n",
    "    #        print ('\\rIteration {}; Predictions: {}; actual: {}\\n'.format(iter_idx, pred_val[0], mat_vals[iter_idx]), end=\"\")\n",
    "            pred_vals[iter_idx][nfeatures_idx] = pred_val[0]\n",
    "            nfeatures_idx += 1\n",
    "        actual.append (mat_vals[iter_idx])\n",
    "        print (\"\\rIteration {}\".format(iter_idx), end=\"\")\n",
    "\n",
    "    print ()\n",
    "    print (\"Calculating R^2\")\n",
    "    r_squareds = [0]*len(feat_range)\n",
    "    nfeatures_idx = 0\n",
    "    for nfeatures in feat_range:\n",
    "        score, p_value = pearsonr(pred_vals[:,nfeatures_idx], actual)\n",
    "        r_squareds[nfeatures_idx] = score * score\n",
    "        print (\"\\rN features {}\".format(nfeatures), end=\"\")\n",
    "        nfeatures_idx += 1\n",
    "    print ()\n",
    "    return (r_squareds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Graph_NFeat_vs_R2 (title, feat_range, avg_score):\n",
    "    pl.figure()\n",
    "    plot_score, = pl.plot(feat_range, avg_score, 'b', label=\"Without LDA\")\n",
    "\n",
    "    best_score_arg = np.nanargmax (avg_score)\n",
    "    best_num_feat = feat_range[best_score_arg]\n",
    "    best_score = avg_score[best_score_arg]\n",
    "\n",
    "    text_x = feat_range[0]+(0.22*(feat_range[-1]-feat_range[0]))\n",
    "    max_y = 0.5\n",
    "    pl.text(text_x,0.9*max_y, 'Max = {:.4g} @ {} features'.format (best_score, best_num_feat))\n",
    "    pl.title(title)\n",
    "    pl.xlabel('Features')\n",
    "    pl.ylabel('Coefficient of Determination '+'( $R^2$)')\n",
    "    pl.ylim([0.0, max_y])\n",
    "    pl.xlim([feat_range[0],feat_range[-1]])\n",
    "#    pl.savefig(os.path.join (Config.graph_dir(),title+'.png'), format='png', dpi=150)\n",
    "    pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def ClsAcc_vs_N_features (classifier, feat_rank_alg, class_mat_list, class_vals, feat_range):\n",
    "    niter = np.product ( [m.shape[0] for m in class_mat_list] )\n",
    "    n_classes = len (class_mat_list)\n",
    "    n_correct = np.zeros( [len(feat_range), n_classes] )\n",
    "    (train,test) = round_robin_iteration (0,class_mat_list)\n",
    "    max_features = train[0].shape[1]\n",
    "    print ('Train class sizes : {}'.format([x.shape[0] for x in train]))\n",
    "    print ('N Features        : {}-{}'.format(feat_range[0],feat_range[-1]))\n",
    "    print ('Iterations        : {}'.format(niter))\n",
    "    print ('class_vals        : {}'.format(class_vals))\n",
    "    for iter_idx in range ( niter ):\n",
    "        # Split\n",
    "        (train,test) = round_robin_iteration (iter_idx,class_mat_list)\n",
    "        (contig_train_mat, contig_train_vals) = list_to_contig_mat (train, class_vals)\n",
    "        (contig_test_mat, contig_test_vals) = list_to_contig_mat (test, class_vals)\n",
    "\n",
    "        # Normalize\n",
    "        (norm_train, norm_test) = normalize (contig_train_mat, contig_test_mat)\n",
    "\n",
    "        # Reduce\n",
    "    #    feature_weights = Pearson (norm_train, contig_train_vals)\n",
    "    #    feature_weights = Fisher (norm_train, contig_train_vals)\n",
    "        feature_weights = feat_rank_alg (norm_train, contig_train_vals)\n",
    "        (sorted_train, sorted_test) = weigh_sort (norm_train, norm_test, feature_weights)\n",
    "\n",
    "        # Classify with different numbers of features\n",
    "    #    preds = rand_forest_clf (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals, rnd_state = iter_idx)\n",
    "    #    preds = WND5_Cls (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals, rnd_state = iter_idx)\n",
    "        nfeatures_idx = 0\n",
    "        for nfeatures in feat_range:\n",
    "            preds = classifier (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals, rnd_state = iter_idx)\n",
    "            for pred_idx in range (len(preds)):\n",
    "                if (preds[pred_idx] == contig_test_vals[pred_idx]):\n",
    "                    n_correct[nfeatures_idx][pred_idx] += 1\n",
    "            cumul_acc = [(float(x) / (float(iter_idx)+1.0)) for x in n_correct[nfeatures_idx]]\n",
    "            print ('\\rIteration {}; cumulative accuracies: {}'.format(iter_idx, cumul_acc), end=\"\")\n",
    "#            print ('Iteration {}; preds: {}, cumul_acc: {}, n_correct[{}]: {}'.format(iter_idx, preds, cumul_acc, nfeatures_idx, n_correct[nfeatures_idx]))\n",
    "            nfeatures_idx += 1\n",
    "    print ()\n",
    "\n",
    "    nfeatures_idx = 0\n",
    "    mean_feat_acc = np.zeros([len(feat_range),n_classes])\n",
    "    for nfeatures in feat_range:\n",
    "        for cl in range (n_classes):\n",
    "            mean_feat_acc[nfeatures_idx][cl] = float(n_correct[nfeatures_idx][cl]) / float(niter)\n",
    "        print (\"\\rN features {}\".format(nfeatures), end=\"\")\n",
    "        nfeatures_idx += 1\n",
    "    print ()\n",
    "\n",
    "    return (mean_feat_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Graph_NFeat_vs_ClsAcc (title, feat_range, mean_feat_acc, class_vals ):\n",
    "    pl.figure()\n",
    "    color_list = pl.cm.Dark2(np.linspace(0, 1, 12))\n",
    "    pl.title(title)\n",
    "    pl.xlabel('Features')\n",
    "    pl.ylabel('Classification Accuracy')\n",
    "    text_y = .8\n",
    "    text_y_drop = 0.1\n",
    "    pl.ylim([0.0, 1.1])\n",
    "    pl.xlim([feat_range[0],feat_range[-1]])\n",
    "\n",
    "    for cls in range(mean_feat_acc.shape[1]):\n",
    "#        plot_score, = pl.plot(feat_range, avg_score, 'b', label=\"Without LDA\")\n",
    "        plot_score, = pl.plot(feat_range, mean_feat_acc[:,cls], color=color_list[cls], label=\"{}\".format(class_vals[cls]))\n",
    "\n",
    "        best_score_arg = np.nanargmax (mean_feat_acc[:,cls])\n",
    "        best_num_feat = feat_range[best_score_arg]\n",
    "        best_score = mean_feat_acc[best_score_arg][cls]\n",
    "\n",
    "        text_x = feat_range[0]+(0.22*(feat_range[-1]-feat_range[0]))\n",
    "        pl.text(text_x,text_y, 'Class {} Max = {:.4g} @ {} features'.format (class_vals[cls],best_score, best_num_feat))\n",
    "        text_y -= text_y_drop\n",
    "\n",
    "#    pl.savefig(os.path.join (Config.graph_dir(),title+'.png'), format='png', dpi=150)\n",
    "    pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def Mean_Feat_weights (rank_alg, class_mat_list, class_vals, feat_names):\n",
    "    niter = np.product ( [m.shape[0] for m in class_mat_list] )\n",
    "    n_classes = len (class_mat_list)\n",
    "    (train,test) = round_robin_iteration (0,class_mat_list)\n",
    "    max_features = train[0].shape[1]\n",
    "    print ('Train class sizes : {}'.format([x.shape[0] for x in train]))\n",
    "    print ('Iterations        : {}'.format(niter))\n",
    "    print ('class_vals        : {}'.format(class_vals))\n",
    "    mean_feature_weights = np.zeros(max_features)\n",
    "    for iter_idx in range ( niter ):\n",
    "        # Split\n",
    "        (train,test) = round_robin_iteration (iter_idx,class_mat_list)\n",
    "        (contig_train_mat, contig_train_vals) = list_to_contig_mat (train, class_vals)\n",
    "        (contig_test_mat, contig_test_vals) = list_to_contig_mat (test, class_vals)\n",
    "\n",
    "        # Normalize\n",
    "        (norm_train, norm_test) = normalize (contig_train_mat, contig_test_mat)\n",
    "\n",
    "        # Reduce\n",
    "    #    feature_weights = Pearson (norm_train, contig_train_vals)\n",
    "    #    feature_weights = Fisher (norm_train, contig_train_vals)\n",
    "        feature_weights = rank_alg (norm_train, contig_train_vals)\n",
    "        mean_feature_weights += feature_weights\n",
    "        print ('\\rIteration {}'.format(iter_idx), end=\"\")\n",
    "    print ()\n",
    "\n",
    "    mean_feature_weights /= iter_idx\n",
    "    i = np.argsort(mean_feature_weights)\n",
    "    i = np.array(list(reversed(i)))\n",
    "    sorted_feature_names = feat_names[i]\n",
    "    sorted_feature_means = mean_feature_weights[i]\n",
    "    sorted_feature_indexes = np.arange(max_features)[i]\n",
    "\n",
    "    return (sorted_feature_names, sorted_feature_indexes, sorted_feature_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ProgressBar\n",
    "```python\n",
    "p = Progressbar(120)\n",
    "for i in range(1, 120+1):\n",
    "    p.animate(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "class ProgressBar(object):\n",
    "    def __init__(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        self.prog_bar = '[]'\n",
    "        self.fill_char = '*'\n",
    "        self.width = 20\n",
    "        self.__update_amount(0)\n",
    "        if have_ipython:\n",
    "            self.animate = self.animate_ipython\n",
    "        else:\n",
    "            self.animate = self.animate_noipython\n",
    "\n",
    "    def animate_ipython(self, iter):\n",
    "        print ('\\r', self, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        self.update_iteration(iter + 1)\n",
    "        if (iter + 1 > self.iterations):\n",
    "            print ()\n",
    "\n",
    "    def update_iteration(self, elapsed_iter):\n",
    "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
    "        self.prog_bar += '  %d of %s complete ' % (elapsed_iter, self.iterations)\n",
    "\n",
    "    def __update_amount(self, new_amount):\n",
    "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
    "        all_full = self.width - 2\n",
    "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
    "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
    "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
    "        pct_string = '%d%%' % percent_done\n",
    "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
    "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.prog_bar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
