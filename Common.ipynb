{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from unidecode import unidecode\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys, time\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_mat_list (mat, mat_labels):\n",
    "    assert (len(mat) == len(mat_labels))\n",
    "    class_label_dict = {}\n",
    "    class_mats = []\n",
    "    class_label_idx = 0\n",
    "    class_labels = []\n",
    "    for samp_idx in range (len (mat)):\n",
    "        class_label = mat_labels[samp_idx]\n",
    "        if not class_label in class_label_dict:\n",
    "            class_label_dict[class_label] = class_label_idx\n",
    "            class_labels.append (class_label)\n",
    "            class_mats.append (mat[samp_idx])\n",
    "            class_label_idx += 1\n",
    "        else:\n",
    "            class_idx = class_label_dict[class_label]\n",
    "            class_mats[class_idx] = np.vstack ([class_mats[class_idx],mat[samp_idx]])\n",
    "    return (np.array(class_mats),class_labels)\n",
    "\n",
    "def list_to_contig_mat (data_matrix_list, class_vals):\n",
    "    data_mat_contig = np.vstack (data_matrix_list)\n",
    "    class_vals_vec_list = []\n",
    "    for class_idx in range (len(data_matrix_list)):\n",
    "        class_vals_vec_list += [class_vals[class_idx]] * len (data_matrix_list[class_idx])\n",
    "    class_vals_contig = np.asarray(class_vals_vec_list)\n",
    "    return (data_mat_contig,class_vals_contig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize (train, test):\n",
    "    norm_train_set = train.copy() \n",
    "    mins, maxs = normalize_by_columns (norm_train_set)\n",
    "    norm_test_set = test.copy() \n",
    "    normalize_by_columns (norm_test_set, mins, maxs)\n",
    "    return (norm_train_set, norm_test_set)\n",
    "\n",
    "def normalize_by_columns ( full_stack, mins = None, maxs = None ):\n",
    "    \"\"\"This is a global function to normalize a matrix by columns.\n",
    "    If numpy 1D arrays of mins and maxs are provided, the matrix will be normalized against these ranges\n",
    "    Otherwise, the mins and maxs will be determined from the matrix, and the matrix will be normalized\n",
    "    against itself. The mins and maxs will be returned as a tuple.\n",
    "    Out of range matrix values will be clipped to min and max (including +/- INF)\n",
    "    zero-range columns will be set to 0.\n",
    "    NANs in the columns will be set to 0.\n",
    "    The normalized output range is hard-coded to 0-100\n",
    "    \"\"\"\n",
    "    # Edge cases to deal with:\n",
    "    # Range determination:\n",
    "    # 1. features that are nan, inf, -inf\n",
    "    # max and min determination must ignore invalid numbers\n",
    "    # nan -> 0, inf -> max, -inf -> min\n",
    "    # Normalization:\n",
    "    # 2. feature values outside of range\n",
    "    # values clipped to range (-inf to min -> min, max to inf -> max) - leaves nan as nan\n",
    "    # 3. feature ranges that are 0 result in nan feature values\n",
    "    # 4. all nan feature values set to 0\n",
    "\n",
    "    # Turn off numpy warnings, since we're taking care of invalid values explicitly\n",
    "    oldsettings = np.seterr(all='ignore')\n",
    "    if (mins is None or maxs is None):\n",
    "        # mask out NANs and +/-INFs to compute min/max\n",
    "        full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
    "        maxs = full_stack_m.max (axis=0)\n",
    "        mins = full_stack_m.min (axis=0)\n",
    "\n",
    "    # clip the values to the min-max range (NANs are left, but +/- INFs are taken care of)\n",
    "    full_stack.clip (mins, maxs, full_stack)\n",
    "    # remake a mask to account for NANs and divide-by-zero from max == min\n",
    "    full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
    "\n",
    "    # Normalize\n",
    "    full_stack_m -= mins\n",
    "    full_stack_m /= (maxs - mins)\n",
    "    # Left over NANs and divide-by-zero from max == min become 0\n",
    "    # Note the deep copy to change the numpy parameter in-place.\n",
    "    full_stack[:] = full_stack_m.filled (0) * 100.0\n",
    "\n",
    "    # return settings to original\n",
    "    np.seterr(**oldsettings)\n",
    "\n",
    "    return (mins,maxs)\n",
    "\n",
    "def standardize (train, test):\n",
    "    scaler = StandardScaler()\n",
    "    new_train_set = scaler.fit_transform(train)\n",
    "    new_test_set = scaler.transform(test)\n",
    "    return (new_train_set,new_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def round_robin_iteration (index, data_matrix_list):\n",
    "    '''Does a leave N out, where N is the number of classes.\n",
    "    The class with the smallest number of samples -1 (nsamples - 1) determines training set size.\n",
    "    Picks nsamples-1 for training and testing from a circular list starting at index.\n",
    "    Index ranges from 0 to the product of number of samples in each class.\n",
    "    data_matrix_list is a list of data matrixes, with one matrix per class'''\n",
    "    lengths = [m.shape[0] for m in data_matrix_list]\n",
    "    nclasses = len(lengths)\n",
    "    max_samples = min (lengths) - 1\n",
    "    indexes = [0] * nclasses\n",
    "    cl_index = index\n",
    "    for i in range (nclasses-1,0,-1):\n",
    "        indexes[i] = cl_index / lengths[i]\n",
    "        cl_index -= (indexes[i] * lengths[i])\n",
    "    indexes[0] = cl_index\n",
    "    indexes = list(reversed(indexes))\n",
    "    \n",
    "    train_mats = []\n",
    "    test_mats = []\n",
    "    for class_num in range(nclasses):\n",
    "        class_indexes = [ (count+indexes[class_num]+1) % lengths[class_num] for count in range (max_samples) ]\n",
    "        train_mats.append (np.take (data_matrix_list[class_num], class_indexes, axis=0) )\n",
    "        test_mats.append (np.take (data_matrix_list[class_num], [indexes[class_num]], axis=0) )\n",
    "    return (train_mats, test_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Fisher(train_mat, train_vals):\n",
    "    \"\"\"Takes a FeatureSet_Discrete as input and calculates a Fisher score for\n",
    "    each feature. Returns a newly instantiated instance of FisherFeatureWeights.\n",
    "\n",
    "    For:\n",
    "    N = number of classes\n",
    "    F = number of features\n",
    "    It = total number of images in training set\n",
    "    Ic = number of images in a given class\n",
    "    \"\"\"\n",
    "\n",
    "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
    "    oldsettings = np.seterr(all='ignore')\n",
    "    (class_mats, class_vals) = get_class_mat_list (train_mat, train_vals)\n",
    "\n",
    "    # 1D matrix 1 * F\n",
    "    population_means = np.mean( train_mat, axis = 0 )\n",
    "    n_classes = class_mats.shape[0]\n",
    "    n_features = train_mat.shape[1]\n",
    "\n",
    "    # 2D matrix shape N * F\n",
    "    intra_class_means = np.empty( [n_classes, n_features] )\n",
    "    # 2D matrix shape N * F\n",
    "    intra_class_variances = np.empty( [n_classes, n_features] )\n",
    "\n",
    "    class_index = 0\n",
    "    for class_feature_matrix in class_mats:\n",
    "        intra_class_means[ class_index ] = np.mean( class_feature_matrix, axis=0 )\n",
    "    # Note that by default, numpy divides by N instead of the more common N-1, hence ddof=1.\n",
    "        intra_class_variances[ class_index ] = np.var( class_feature_matrix, axis=0, ddof=1 )\n",
    "        class_index += 1\n",
    "\n",
    "    # 1D matrix 1 * F\n",
    "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
    "    # for the record, in numpy:\n",
    "    # 1./0. = inf, 0./inf = 0., 1./inf = 0. inf/0. = inf, inf/inf = nan\n",
    "    # 0./0. = nan, nan/0. = nan, 0/nan = nan, nan/nan = nan, nan/inf = nan, inf/nan = nan\n",
    "    # We can't deal with NANs only, must also deal with pos/neg infs\n",
    "    # The masked array allows for dealing with \"invalid\" floats, which includes nan and +/-inf\n",
    "    denom = np.mean( intra_class_variances, axis = 0 )\n",
    "    denom[denom == 0] = np.nan\n",
    "    feature_weights_m = np.ma.masked_invalid (\n",
    "            ( np.square( population_means - intra_class_means ).sum( axis = 0 ) /\n",
    "        (n_classes - 1) ) / denom\n",
    "        )\n",
    "    # return numpy error settings to original\n",
    "    np.seterr(**oldsettings)\n",
    "\n",
    "    # the filled(0) method of the masked array sets all nan and infs to 0\n",
    "    fisher_values = feature_weights_m.filled(0).tolist()\n",
    "\n",
    "    return (fisher_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pearson(train_mat, train_vals):\n",
    "    \"\"\"Calculate regression parameters and correlation statistics that fully define\n",
    "    a continuous classifier.\n",
    "\n",
    "    At present the feature weights are proportional the Pearson correlation coefficient\n",
    "    for each given feature.\"\"\"\n",
    "\n",
    "    from scipy import stats\n",
    "\n",
    "    # Known issue: running stats.linregress() with np.seterr (all='raise') has caused\n",
    "    # arithmetic underflow (FloatingPointError: 'underflow encountered in stdtr' )\n",
    "    # I think this is something we can safely ignore in this function, and return settings\n",
    "    # back to normal at the end. -CEC\n",
    "    np.seterr (under='ignore')    \n",
    "\n",
    "    pearson_coeffs = np.zeros(train_mat.shape[1])\n",
    "\n",
    "    for feature_index in range( train_mat.shape[1] ):\n",
    "        slope, intercept, pearson_coeff, p_value, std_err = stats.linregress(\n",
    "            train_vals, train_mat[:,feature_index]\n",
    "        )\n",
    "\n",
    "        pearson_coeffs[feature_index] = pearson_coeff\n",
    "# We're just returning the pearsons^2 now...\n",
    "#    pearson_values = [val*val / r_val_squared_sum for val in pearson_coeffs ]\n",
    "#    pearson_coeffs = (pearson_coeffs * pearson_coeffs) / r_val_squared_sum\n",
    "    pearson_coeffs *= pearson_coeffs\n",
    "    \n",
    "\n",
    "    # Reset numpy\n",
    "    np.seterr (all='raise')\n",
    "\n",
    "    return pearson_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_by_weight (the_mat, feature_weights):\n",
    "    i = np.argsort(feature_weights)\n",
    "    sort_mat = the_mat[:,i]\n",
    "    sort_mat = np.fliplr(sort_mat)\n",
    "    return (sort_mat)\n",
    "\n",
    "def weigh_sort(train, test, feature_weights):\n",
    "    weigh_train = np.multiply (train, feature_weights)\n",
    "    weigh_test = np.multiply (test, feature_weights)\n",
    "\n",
    "    sorted_train = sort_by_weight (weigh_train, feature_weights)\n",
    "    sorted_test = sort_by_weight (weigh_test, feature_weights)\n",
    "    return (sorted_train, sorted_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def marg_prob_to_pred_value (marg_probs, class_vals):\n",
    "    weighted = np.array(marg_probs)*np.array(class_vals)\n",
    "    return (np.sum(weighted))\n",
    "\n",
    "def WND5(contig_train_mat, contig_test_mat, contig_train_vals):\n",
    "    n_test_samples = contig_test_mat.shape[0]\n",
    "    n_train_samples = contig_train_mat.shape[0]\n",
    "    predicted_classes = np.zeros(n_test_samples)\n",
    "    predicted_values = np.zeros(n_test_samples)\n",
    "    \n",
    "    epsilon = np.finfo( np.float ).eps\n",
    "    testimg_idx = 0\n",
    "    trainimg_idx = 0\n",
    "    \n",
    "    for testimg_idx in range( n_test_samples ):\n",
    "        # initialize\n",
    "        class_dists = {}\n",
    "        class_counts = {}\n",
    "        classnames_list = []\n",
    "\n",
    "        for trainimg_idx in range( n_train_samples ):\n",
    "            train_class_label = contig_train_vals[trainimg_idx]\n",
    "            if not train_class_label in class_dists:\n",
    "                class_dists [train_class_label] = 0.0\n",
    "                class_counts[train_class_label] = 0.0\n",
    "                classnames_list.append (train_class_label)\n",
    "\n",
    "            dists = np.absolute (contig_train_mat [trainimg_idx] - contig_test_mat [testimg_idx])\n",
    "            w_dist = np.sum( dists )\n",
    "            if w_dist > epsilon:\n",
    "                class_counts[train_class_label] += 1.0\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            w_dist = np.sum( np.square( dists ) )\n",
    "            # The exponent -5 is the \"5\" in \"WND5\"\n",
    "            class_dists[ train_class_label ] += w_dist ** -5\n",
    "\n",
    "        \n",
    "        class_idx = 0\n",
    "        class_similarities = [0]*len(class_dists)\n",
    "        for class_label in classnames_list:\n",
    "            class_similarities[class_idx] = class_dists[class_label] / class_counts[class_label]\n",
    "            class_idx += 1\n",
    "\n",
    "        norm_factor = sum( class_similarities )\n",
    "        marg_probs = np.array( [ x / norm_factor for x in class_similarities ] )\n",
    "\n",
    "        predicted_class_idx = marg_probs.argmax()\n",
    "\n",
    "        predicted_classes[testimg_idx] = classnames_list[ predicted_class_idx ]\n",
    "        predicted_values[testimg_idx] = marg_prob_to_pred_value (marg_probs, classnames_list)\n",
    "\n",
    "    return (predicted_classes, predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_forest_clf (contig_train_mat, contig_test_mat, contig_train_vals, rnd_state = None):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators = 30, random_state = rnd_state)\n",
    "    clf.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted_classes = clf.predict(contig_test_mat)\n",
    "    return (predicted_classes)\n",
    "def rand_forest_reg (contig_train_mat, contig_test_mat, contig_train_vals, rnd_state = None):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    forest = RandomForestRegressor(n_estimators=30, random_state = rnd_state)\n",
    "    forest.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted = forest.predict(contig_test_mat)\n",
    "    return (predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lin_reg(contig_train_mat, contig_test_mat, contig_train_vals):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted = lin_reg.predict(contig_test_mat)\n",
    "    return (predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ProgressBar\n",
    "```python\n",
    "p = Progressbar(120)\n",
    "for i in range(1, 120+1):\n",
    "    p.animate(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "class ProgressBar(object):\n",
    "    def __init__(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        self.prog_bar = '[]'\n",
    "        self.fill_char = '*'\n",
    "        self.width = 20\n",
    "        self.__update_amount(0)\n",
    "        if have_ipython:\n",
    "            self.animate = self.animate_ipython\n",
    "        else:\n",
    "            self.animate = self.animate_noipython\n",
    "\n",
    "    def animate_ipython(self, iter):\n",
    "        print ('\\r', self, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        self.update_iteration(iter + 1)\n",
    "        if (iter + 1 > self.iterations):\n",
    "            print ()\n",
    "\n",
    "    def update_iteration(self, elapsed_iter):\n",
    "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
    "        self.prog_bar += '  %d of %s complete ' % (elapsed_iter, self.iterations)\n",
    "\n",
    "    def __update_amount(self, new_amount):\n",
    "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
    "        all_full = self.width - 2\n",
    "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
    "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
    "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
    "        pct_string = '%d%%' % percent_done\n",
    "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
    "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.prog_bar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
